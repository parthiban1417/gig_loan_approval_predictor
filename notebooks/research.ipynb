{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'gigloan (Python 3.8.20)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n gigloan ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import poisson\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Number of samples\n",
    "n = 5000\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Basic Applicant Information\n",
    "# ---------------------------\n",
    "applicant_id = np.arange(1, n + 1)\n",
    "age = np.random.randint(22, 60, n)\n",
    "education_level = np.random.choice(\n",
    "    [\"High School\", \"Graduate\", \"Postgraduate\"],\n",
    "    n,\n",
    "    p=[0.4, 0.45, 0.15]\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Gig Work Details\n",
    "# ---------------------------\n",
    "# List of popular gig platforms in India\n",
    "all_platforms = [\n",
    "    \"Swiggy\", \"Zomato\", \"Rapido\", \"Ola\", \"Uber\",\n",
    "    \"Amazon Flex\", \"Dunzo\", \"UrbanClap\", \"Fiverr\", \"Upwork\"\n",
    "]\n",
    "\n",
    "def pick_platforms():\n",
    "    count = np.random.choice([1, 2, 3], p=[0.6, 0.3, 0.1])\n",
    "    return random.sample(all_platforms, count)\n",
    "\n",
    "gig_platforms_list = [pick_platforms() for _ in range(n)]\n",
    "gig_platforms = [\", \".join(platforms) for platforms in gig_platforms_list]\n",
    "num_platforms = [len(platforms) for platforms in gig_platforms_list]\n",
    "work_experience = np.random.randint(0, 15, n)  # in years\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Financial Information with Seasonal Effects\n",
    "# ---------------------------\n",
    "# Base monthly income using a normal distribution (in INR)\n",
    "base_income = np.random.normal(loc=30000, scale=10000, size=n).astype(int)\n",
    "base_income = np.clip(base_income, 5000, 100000)\n",
    "\n",
    "# Seasonal variation factor: simulate peaks (range from 0.8 to 1.5)\n",
    "seasonal_variation = np.random.choice(np.linspace(0.8, 1.5, 15), n)\n",
    "\n",
    "# Adjust monthly income for multi-platform engagement (10% boost per extra platform)\n",
    "monthly_income = (base_income * seasonal_variation * (1 + 0.1 * (np.array(num_platforms) - 1))).astype(int)\n",
    "\n",
    "# Income volatility: proportional to income (5% to 30% variability)\n",
    "income_volatility = (monthly_income * np.random.uniform(0.05, 0.3, n)).astype(int)\n",
    "\n",
    "# Savings balance: roughly 6 months of income times a factor between 0.2 and 0.5\n",
    "savings_balance = (monthly_income * 6 * np.random.uniform(0.2, 0.5, n)).astype(int)\n",
    "savings_balance = np.clip(savings_balance, 0, 500000)\n",
    "\n",
    "# Simulate existing loans realistically\n",
    "existing_loans = np.random.choice([0, 1, 2, 3], size=n, p=[0.7, 0.2, 0.08, 0.02])\n",
    "\n",
    "# Calculate Debt-to-Income Ratio (DTI)\n",
    "dti_base = np.random.uniform(0.1, 0.3, n)\n",
    "debt_to_income_ratio = np.clip(dti_base * (1 + 0.2 * existing_loans), 0.1, 0.6)\n",
    "\n",
    "# Loan amount requested (in INR) with microfinance focus: 70% micro (below 50k), 30% regular\n",
    "micro_loans = np.random.randint(10000, 50000, int(n * 0.7))\n",
    "regular_loans = np.random.randint(50000, 500000, n - int(n * 0.7))\n",
    "loan_amount_requested = np.concatenate([micro_loans, regular_loans])\n",
    "np.random.shuffle(loan_amount_requested)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Credit Score Calculation (CIBIL-like)\n",
    "# ---------------------------\n",
    "credit_score = (\n",
    "    300 +\n",
    "    (monthly_income / 100) -\n",
    "    (debt_to_income_ratio * 50) +\n",
    "    np.random.normal(0, 30, n)\n",
    ")\n",
    "credit_score = np.clip(credit_score, 300, 900).astype(int)\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Behavioral and Economic Factors\n",
    "# ---------------------------\n",
    "transaction_frequency = np.random.randint(10, 100, n)\n",
    "avg_monthly_expenses = np.random.randint(5000, 70000, n)\n",
    "credit_card_utilization = np.random.randint(10, 90, n)\n",
    "subscription_services = np.random.randint(0, 5, n)\n",
    "financial_emergencies_last_year = np.random.randint(0, 5, n)\n",
    "# Enhanced inflation rates (capped at 7.8% per RBI data)\n",
    "inflation_rate = np.round(np.random.uniform(3, 7.8, n), 2)\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Additional Untraditional Parameters\n",
    "# ---------------------------\n",
    "loan_reasons = [\n",
    "    \"Vehicle Purchase\", \"Medical Emergency\", \"Education\", \n",
    "    \"Home Renovation\", \"Debt Consolidation\", \"Business Expansion\", \"Other\"\n",
    "]\n",
    "reason_for_loan = np.random.choice(loan_reasons, n)\n",
    "\n",
    "def generate_platform_ratings(platforms):\n",
    "    ratings = {p: round(np.random.uniform(3.0, 5.0), 1) for p in platforms}\n",
    "    return \"; \".join([f\"{p}:{r}\" for p, r in ratings.items()])\n",
    "\n",
    "platform_ratings = [generate_platform_ratings(platforms) for platforms in gig_platforms_list]\n",
    "customer_feedback_score = [\n",
    "    round(np.mean([float(r.split(\":\")[1]) for r in ratings.split(\"; \")]) * 20 + np.random.uniform(-5, 5), 1)\n",
    "    for ratings in platform_ratings\n",
    "]\n",
    "work_consistency = np.random.randint(1, 8, n)\n",
    "penalties = [max(0, int(np.random.poisson(1) - (score / 100))) for score in customer_feedback_score]\n",
    "alternative_income_source = np.random.choice([\"Yes\", \"No\"], n, p=[0.3, 0.7])\n",
    "loan_coapplicant = np.random.choice([\"Yes\", \"No\"], n, p=[0.2, 0.8])\n",
    "\n",
    "# ---------------------------\n",
    "# 7. Enhanced Geographic and Additional Features\n",
    "# ---------------------------\n",
    "indian_states = [\n",
    "    'Maharashtra', 'Karnataka', 'Delhi', 'Tamil Nadu', 'Uttar Pradesh',\n",
    "    'Gujarat', 'West Bengal', 'Telangana', 'Rajasthan', 'Bihar'\n",
    "]\n",
    "location = np.random.choice(indian_states, n, p=[0.18, 0.15, 0.12, 0.1, 0.1, 0.09, 0.08, 0.07, 0.06, 0.05])\n",
    "urban_ratio = 0.35\n",
    "urban_rural = np.random.choice(['Urban', 'Rural'], n, p=[urban_ratio, 1 - urban_ratio])\n",
    "\n",
    "def generate_platform_tenure(platforms):\n",
    "    return [random.randint(3, 60) for _ in platforms]\n",
    "\n",
    "platform_tenures = [generate_platform_tenure(platforms) for platforms in gig_platforms_list]\n",
    "avg_platform_tenure = [np.mean(tenures) for tenures in platform_tenures]\n",
    "\n",
    "family_dependents = poisson.rvs(mu=1.5, size=n)\n",
    "family_dependents = np.clip(family_dependents, 0, 5)\n",
    "\n",
    "cost_of_living_index = {\n",
    "    'Maharashtra': 1.15, 'Karnataka': 1.1, 'Delhi': 1.25,\n",
    "    'Tamil Nadu': 1.05, 'Uttar Pradesh': 0.95, 'Gujarat': 1.0,\n",
    "    'West Bengal': 0.9, 'Telangana': 1.07, 'Rajasthan': 0.93, 'Bihar': 0.85\n",
    "}\n",
    "cost_of_living = np.array([cost_of_living_index[state] for state in location])\n",
    "\n",
    "# ---------------------------\n",
    "# 8. Loan Approval Outcome with Logical Checks\n",
    "# ---------------------------\n",
    "def determine_loan_approval(i):\n",
    "    # Basic financial criteria\n",
    "    crit_credit = credit_score[i] > 650\n",
    "    crit_dti = debt_to_income_ratio[i] < 0.4\n",
    "    crit_savings = savings_balance[i] > (loan_amount_requested[i] * 0.2)\n",
    "    crit_feedback = customer_feedback_score[i] > 70\n",
    "    crit_consistency = work_consistency[i] >= 3\n",
    "\n",
    "    # Logical checks:\n",
    "    crit_min_credit = credit_score[i] >= 500  # Very low credit scores should be rejected\n",
    "\n",
    "    # If no co-applicant, require a higher credit score\n",
    "    if loan_coapplicant[i] == \"No\":\n",
    "        crit_coapplicant = credit_score[i] > 700\n",
    "    else:\n",
    "        crit_coapplicant = True\n",
    "\n",
    "    # Combine all criteria\n",
    "    if all([crit_credit, crit_dti, crit_savings, crit_feedback, crit_consistency, \n",
    "            crit_min_credit, crit_coapplicant]):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "loan_approved = [determine_loan_approval(i) for i in range(n)]\n",
    "\n",
    "# ---------------------------\n",
    "# 9. Create Final DataFrame and Introduce Missing Values\n",
    "# ---------------------------\n",
    "df = pd.DataFrame({\n",
    "    \"applicant_id\": applicant_id,\n",
    "    \"age\": age,\n",
    "    \"education_level\": education_level,\n",
    "    \"gig_platforms\": gig_platforms,\n",
    "    \"num_platforms\": num_platforms,\n",
    "    \"work_experience\": work_experience,\n",
    "    \"monthly_income\": monthly_income,\n",
    "    \"seasonal_variation\": seasonal_variation,\n",
    "    \"income_volatility\": income_volatility,\n",
    "    \"savings_balance\": savings_balance,\n",
    "    \"debt_to_income_ratio\": debt_to_income_ratio,\n",
    "    \"credit_score\": credit_score,\n",
    "    \"existing_loans\": existing_loans,\n",
    "    \"loan_amount_requested\": loan_amount_requested,\n",
    "    \"transaction_frequency\": transaction_frequency,\n",
    "    \"avg_monthly_expenses\": avg_monthly_expenses,\n",
    "    \"credit_card_utilization\": credit_card_utilization,\n",
    "    \"subscription_services\": subscription_services,\n",
    "    \"financial_emergencies_last_year\": financial_emergencies_last_year,\n",
    "    \"inflation_rate\": inflation_rate,\n",
    "    \"reason_for_loan\": reason_for_loan,\n",
    "    \"platform_ratings\": platform_ratings,\n",
    "    \"customer_feedback_score\": customer_feedback_score,\n",
    "    \"work_consistency\": work_consistency,\n",
    "    \"penalties\": penalties,\n",
    "    \"alternative_income_source\": alternative_income_source,\n",
    "    \"loan_coapplicant\": loan_coapplicant,\n",
    "    \"location\": location,\n",
    "    \"urban_rural\": urban_rural,\n",
    "    \"avg_platform_tenure\": avg_platform_tenure,\n",
    "    \"family_dependents\": family_dependents,\n",
    "    \"cost_of_living_index\": cost_of_living,\n",
    "    \"loan_approved\": loan_approved\n",
    "})\n",
    "\n",
    "# ---------------------------\n",
    "# 10. Introduce Missing Values (Enhanced Pattern)\n",
    "# ---------------------------\n",
    "missing_config = {\n",
    "    'education_level': 0.08,\n",
    "    'work_experience': 0.08,\n",
    "    'monthly_income': 0.12,\n",
    "    'savings_balance': 0.12,\n",
    "    'credit_score': 0.12,\n",
    "    'avg_monthly_expenses': 0.12,\n",
    "    'urban_rural': 0.05,\n",
    "    'family_dependents': 0.03\n",
    "}\n",
    "\n",
    "for col, ratio in missing_config.items():\n",
    "    df.loc[df.sample(frac=ratio).index, col] = np.nan\n",
    "\n",
    "# ---------------------------\n",
    "# 11. Validation Checks (Sanity Checks)\n",
    "# ---------------------------\n",
    "# 1. Inflation rate sanity check (should not exceed 7.8%)\n",
    "assert df.inflation_rate.max() <= 7.8, \"Inflation rate exceeds RBI cap.\"\n",
    "\n",
    "# 2. Micro-loans proportion: ~70% loans should be under â‚¹50k\n",
    "micro_proportion = (df.loan_amount_requested < 50000).mean()\n",
    "assert 0.65 <= micro_proportion <= 0.75, \"Micro-loan proportion out of range.\"\n",
    "\n",
    "# 3. Regional cost of living: sample check for Delhi (if any records exist)\n",
    "if (df.location == 'Delhi').any():\n",
    "    delhi_cost = df[df.location == 'Delhi'].cost_of_living_index.mean()\n",
    "    assert 1.2 < delhi_cost < 1.3, \"Delhi cost of living anomaly.\"\n",
    "\n",
    "# ---------------------------\n",
    "# 12. Save Final Enhanced Dataset\n",
    "# ---------------------------\n",
    "df.to_csv(\"C:/test/gig_loan_approval_predictor/Artifacts/indian_gig_loan_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['work_experience'] == 0) & (df['monthly_income'].isnull()), 'loan_approved'] = 0\n",
    "# Create a fraud flag column\n",
    "df['fraud_flag'] = ((df['work_experience'] == 0) & (df['monthly_income'].isnull())).astype(int)\n",
    "#those who have work experience and monthly income as 0 are frauds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into fraud and non-fraud\n",
    "non_fraud_data = df[df['fraud_flag'] == 0]\n",
    "fraud_data = df[df['fraud_flag'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values as 0 in monthly_income the fraud data\n",
    "fraud_data['monthly_income'] = fraud_data['monthly_income'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with median in the non-fraud data\n",
    "non_fraud_data['monthly_income'].fillna(non_fraud_data['monthly_income'].median(), inplace=True)\n",
    "non_fraud_data['work_experience'].fillna(non_fraud_data['work_experience'].median(), inplace=True)\n",
    "non_fraud_data[non_fraud_data['work_experience']==0].fillna(non_fraud_data['work_experience'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([non_fraud_data,fraud_data],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a flag for first-time applicants (no work experience, no existing loans)\n",
    "df['first_time_applicant'] = ((df['work_experience'] == 0) & (df['existing_loans'] == 0)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set credit score to -1 for first-time applicants\n",
    "df.loc[df['first_time_applicant'] == 1, 'credit_score'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in the savings _balence, avg_monthly_expenses and credit_score with the median\n",
    "df['savings_balance'].fillna(df['savings_balance'].median(), inplace=True)\n",
    "df['avg_monthly_expenses'].fillna(df['avg_monthly_expenses'].median(), inplace=True)\n",
    "df['credit_score'].fillna(df['credit_score'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in the urban_rural, family_dependents and education_level columns with the mode\n",
    "df['urban_rural'].fillna(df['urban_rural'].mode()[0], inplace=True)\n",
    "df['family_dependents'].fillna(df['family_dependents'].mode()[0], inplace=True)\n",
    "df['education_level'].fillna(df['education_level'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the string into key-value pairs\n",
    "platform_ratings = df['platform_ratings'].str.split('; ', expand=True)\n",
    "\n",
    "# Convert to a dictionary using `str.split`\n",
    "platform_ratings_dict = platform_ratings.applymap(lambda x: dict([x.split(':')]) if pd.notna(x) else {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform_ratings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the average of dictionary values\n",
    "def calculate_avg(ratings):\n",
    "    if isinstance(ratings, dict) and len(ratings) > 0:\n",
    "        return sum(map(float, ratings.values())) / len(ratings)\n",
    "    return None\n",
    "\n",
    "# Create a new column with the average rating\n",
    "platform_ratings_dict['avg_platform_rating'] = platform_ratings_dict[[0,1,2]].applymap(calculate_avg).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform_ratings_dict.drop([0,1,2], axis=1, inplace=True)\n",
    "platform_ratings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df, platform_ratings_dict], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"C:/test/gig_loan_approval_predictor/Artifacts/gig_loan_processed_analysis.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['first_time_applicant','fraud_flag','applicant_id','gig_platforms','platform_ratings','location'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#label encoding categorical columns\n",
    "cat=df.select_dtypes(include='object')\n",
    "for i in cat:\n",
    "    df[i]=df[i].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation=df.corr()['loan_approved'].to_frame()\n",
    "plt.figure(figsize=(40, 30))\n",
    "sns.heatmap(correlation, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding for the categorical columns\n",
    "df['education_level'] = df['education_level'].map({'High School': 0, 'Graduate': 1, 'Postgraduate': 2})\n",
    "df['urban_rural'] = df['urban_rural'].map({'Urban': 1, 'Rural': 0})\n",
    "df['alternative_income_source'] = df['alternative_income_source'].map({'Yes': 1, 'No': 0})\n",
    "df['loan_coapplicant'] = df['loan_coapplicant'].map({'Yes': 1, 'No': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['education_level']=df['education_level'].astype('int')\n",
    "df['alternative_income_source']=df['alternative_income_source'].astype('int')\n",
    "df['loan_coapplicant']=df['loan_coapplicant'].astype('int')\n",
    "df['urban_rural']=df['urban_rural'].astype('int')\n",
    "df['family_dependents']=df['family_dependents'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding \n",
    "df = pd.get_dummies(df, columns=['reason_for_loan'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['savings_balance'] = np.log1p(df['savings_balance'])\n",
    "df['existing_loans'] = np.log1p(df['existing_loans'])\n",
    "df['loan_amount_requested'] = np.log1p(df['loan_amount_requested'])\n",
    "df['penalties'] = np.log1p(df['penalties'])\n",
    "df['num_platforms'] = np.log1p(df['num_platforms'])\n",
    "df['loan_coapplicant'] = np.log1p(df['loan_coapplicant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import PowerTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('loan_approved', axis=1) \n",
    "y = df['loan_approved']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y)\n",
    "\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "X_train['credit_score'] = pt.fit_transform(X_train[['credit_score']])\n",
    "X_test['credit_score'] = pt.transform(X_test[['credit_score']])\n",
    "\n",
    "\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, \n",
    "                            max_depth=10, \n",
    "                            random_state=42, \n",
    "                            )\n",
    "\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gigloan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
